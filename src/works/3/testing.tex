% !TEX TS-program = lualatex
В рамках тестирования были созданы две модели с различными гиперпараметрами.

\begin{df}{|C|C|}{m}{Тестирование моделей с различными гиперпараметрами}{test-diff-params}\hline
    Модель & Число нейронов \\ \hline
    TD-Gammon Версия 1 & $40$ \\ \hline
    TD-Gammon Версия 2 & $80$ \\ \hline
\end{df}

Основное различие между моделями (\refdf{test-diff-params}) заключается в числе нейронов на скрытом слое нейронной сети. Для оценки влияния этого параметра на процесс обучения проанализируем динамику развития моделей в зависимости от числа нейронов в скрытом слое.

\image{Сравнение двух моделей по победам за 10,000 игр}{model-comparing}{model-comparing}

На графике видно, что в начальной фазе обучения (первые $\sim$1000 матчей) соотношение числа побед второй модели к первой колебалось вокруг единицы. Это свидетельствует о примерно равных шансах на победу обеих моделей на ранних этапах обучения, когда стратегия игры ещё не была достаточно развитой.

Однако с увеличением количества сыгранных матчей модель с увеличенным числом нейронов стала демонстрировать более эффективную адаптацию к условиям среды, что выразилось в уверенном росте числа её побед на дистанции в 10,000 игр.

Тем не менее, следует подчеркнуть, что несмотря на двукратное увеличение размера скрытого слоя, рост числа побед оказался не пропорциональным увеличению числа нейронов. Это может указывать на наличие ограничений в способности модели использовать дополнительные вычислительные ресурсы без соответствующей настройки архитектуры сети или параметров обучения.
