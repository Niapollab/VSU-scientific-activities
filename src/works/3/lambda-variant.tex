% !TEX TS-program = lualatex
Значительный вклад в развитие метода временной разницы сделал Ричард Саттон, который в 1988 году представил алгоритм $TD(\lambda)$ — один из первых методов, использующих этот подход для решения задач обучения с подкреплением \cite{td-first-improvement}.

Предложенный алгоритм $TD(\lambda)$ является обобщением уравнения (\ref{td-base}) и включает использование следов пригодности — расширения, позволяющего агенту обновлять не только значение текущего состояния, но и значения предшествующих состояний вдоль траектории. Параметр $\lambda$ регулирует степень влияния более ранних состояний на обновления.

Правило обновления для $TD(\lambda)$ задается следующим образом:

\begin{equation}\label{td-lambda}
    V(s_t) \leftarrow V(s_t) + \alpha \delta_t \cdot \lambda^{t - k}
\end{equation}

Где $\lambda$ --- параметр, который определяет, как далеко в прошлое должны распространяться следы пригодности, $\lambda \in [0, 1]$.

След $\lambda^{t-k}$ убывает с течением времени, а степень влияния предыдущих состояний контролируется параметром $\lambda$. При $t-k = 1$ метод становится аналогом метода Монте-Карло, а при $t-k = 0$ соответствует стандартному методу $TD(0)$.
