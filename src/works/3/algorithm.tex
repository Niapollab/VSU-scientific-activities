% !TEX TS-program = lualatex

\subsubsection{Инициализация}
Алгоритм начинается с инициализации вектора весов $\vec{\theta}$ произвольным образом.

\subsubsection{Цикл эпизодов}
Алгоритм эмулирует партию игры в <<Нарды>>. Каждый эпизод начинается с начального состояния $s$, вектор следов $\vec{e}$ инициализируется нулями.

\subsubsection{Цикл шагов (внутри эпизода)}
\begin{itemize}
    \item \textit{Выбор действия}. На каждом шаге выбирается действие $a$ в соответствии с правилами эмуляции системы $\pi$ для текущего состояния $s$.
    \item \textit{Наблюдение}. После выполнения действия агент получает вознаграждение $r$ и переходит в новое состояние $s'$.
    \item \textit{Стандартная ошибка алгоритма}. Алгоритм вычисляет стандартную ошибку (временных разностей) $\delta$, которая является разницей между ожидаемым вознаграждением (включая текущее вознаграждение $r$, а также дисконтированное значение следующего состояния $V(s')$) и текущей оценкой $V(s)$ для состояния $s$.
    \item \textit{Обновление следа}. Вектор следов $\vec{e}$ обновляется по формуле:
    \begin{equation}
        \vec{e} = \gamma \lambda \vec{e} + \nabla_{\vec{\theta}} V(s),
    \end{equation}
    где $\gamma$ --- коэффициент дисконтирования, а $\lambda$ --- контролирует затухание следа.
    \item \textit{Обновление параметров}. Вектор параметров $\vec{\theta}$ обновляется по направлению градиента функции ценности относительно $\vec{\theta}$, взвешенному на стандартную ошибку алгоритма $\delta$ и след $\vec{e}$. Это обновление осуществляется с помощью метода градиентного спуска \cite{gradient-descent}, который позволяет итеративно минимизировать ошибку и улучшать параметры $\vec{\theta}$:
    \begin{equation}
        \vec{\theta} \gets \vec{\theta} + \alpha \delta \vec{e},
    \end{equation}
    где $\alpha$ — коэффициент обучения, который контролирует размер шага обновления параметров. Этот шаг позволяет уточнять локальный минимум функции стандартной ошибки, улучшая производительность агента.
    \item \textit{Переход к следующему состоянию}. Агент переходит в следующее состояние $s' = s$.
    \end{itemize}

\subsubsection{Завершение}
Внутренний цикл для каждого эпизода продолжается до достижения конечного состояния. В частом случае --- это число эпизодов.

Таким образом, алгоритм может быть представлен в виде:

\begin{alg}{Обучение с временными разностями с учётом следов}{rl-algorithm}
    \State Инициализировать $\vec{\theta}$ произвольным образом
    \While{не завершены все эпизоды}
        \State $\vec{e} \gets \vec{0}$
        \State $s \gets$ начальное состояние эпизода
        \While{$s$ не является конечным}
            \State $a \gets$ действие, выбранное по $\pi$ для $s$
            \State Выполнить действие $a$, наблюдать вознаграждение $r$ и следующее состояние $s'$
            \State $\delta \gets r + \gamma V(s') - V(s)$
            \State $\vec{e} \gets \gamma \lambda \vec{e} + \nabla_{\vec{\theta}} V(s)$
            \State $\vec{\theta} \gets \vec{\theta} + \alpha \delta \vec{e}$
            \State $s \gets s'$
        \EndWhile
    \EndWhile
\end{alg}
