% !TEX TS-program = lualatex
TD-Gammon, сокращение от Temporal Difference Gammon, является знаковым достижением в области искусственного интеллекта и настольных игр в частности. Разработанный Джеральдом Тесауро в начале 1990-х годов, TD-Gammon продемонстрировал потенциал обучения нейронных сетей для освоения сложных стратегических игр.

В отличии от конкурентов в лице шахматных компьютеров того времени, искусственный интеллект TD-Gammon не анализировал дерево игры в глубину, а использовал обучение с подкреплением.

Идея обучения с подкреплением отличается от классической модели обучения с учителем. $i$-ым шагом обучения является выбор некоторого хода $a_i \in A$, где $A$ --- является множеством всех доступных ходов для некоторого состояния $s_i \in S$, где $S$ --- определяет множество всех доступных состояний.

В роли учителя выступает некоторая <<окружающая среда>>, оценивающая $a_i$-ый ход на основе $s_i$-ого состояния. Таким образом, находясь в состоянии $s_i$ искусственный интеллект выполняет поиск $a_i$-го хода, согласно некоторой оценке $r_i$ и получает от окружающей среды новую оценку $r_{i + 1}$, переходя в состояние $s_{i + 1}$ (\refimage{tdschema}).

\image{Схематичное представление процесса обучения с подкреплением}{td-schema}{tdschema}

В процессе реализации идеи возникают такие вопросы как: <<Что считать состоянием?>>, <<Как вычислить оценку?>>, <<Как распространить оценку от конечного состояния к начальному?>>. Существующие методы ответа на эти вопросы влияют на конечный результат, зависят от задачи и описаны в научных трудах \cite{tdlearn}.

Благодаря обширной самостоятельной игре и обучению на своих ошибках, искусственный интеллект достиг уровня игры, который превзошел уровень гроссмейстеров-людей. В 1992 году TD-Gammon выиграла чемпионат мира по нардам, продемонстрировав свое мастерство и стратегическое понимание игры.
