% !TEX TS-program = lualatex
Одним из главных преимуществ полных игровых деревьев является возможность создать непобедимый искусственный интеллект. В начале десятилетия было успешно построено полное дерево для игры шашки \cite{checkersolved}.

Основной проблемой для разработки полных деревьев становится их экспоненциальный рост, поскольку каждый ход увеличивает число возможных исходов. В \refdf{gamedefparams} представлены мощности полных деревьев для некоторых популярных игр.

\begin{df}{|C|C|C|C|}{m}{Мощности полных деревьев для некоторых популярных игр}{gamedefparams}\hline
    Название игры & Степень ветвления дерева $b$ & Глубина дерева игры $d$ & Мощность дерева $b^d$ \\ \hline
    Крестики-нолики & $4$ & $9$ & $262144$  \\ \hline
    Шашки & $2.8$ & $70$ & $2 \times 10^{31}$  \\ \hline
    Шахматы & $35$ & $70$ & $1.22 \times 10^{128}$  \\ \hline
    Нарды & $250$ & $55$ & $7.70 \times 10^{131}$ \\ \hline
    Го & $250$ & $150$ & $4.91 \times 10^{359}$ \\ \hline
\end{df}

Это затрудняет полный анализ или представление всего дерева, особенно в сложных играх с большим количеством возможных ходов и исходов. Одним из возможных решений данной проблемы является аппроксимацию частей дерева, чтобы сделать вычисления выполнимыми. Однако это может привести к неоптимальным стратегиям или упущению важных игровых состояний.

TD-Gammon, сокращение от Temporal Difference Gammon, является знаковым достижением в области искусственного интеллекта и настольных игр в частности. Разработанный Джеральдом Тесауро в начале 1990-х годов, TD-Gammon продемонстрировал потенциал обучения нейронных сетей для освоения сложных стратегических игр.

В отличии от конкурентов в лице шахматных компьютеров того времени, искусственный интеллект TD-Gammon не анализировал дерево игры в глубину, а использовал обучение с подкреплением.

Идея обучения с подкреплением отличается от классической модели обучения с учителем. $i$-ым шагом обучения является выбор некоторого хода $a_i \in A$, где $A$ --- является множеством всех доступных ходов для некоторого состояния $s_i \in S$, где $S$ --- определяет множество всех доступных состояний.

В роли учителя выступает некоторая <<окружающая среда>>, оценивающая $a_i$-ый ход на основе $s_i$-ого состояния. Таким образом, находясь в состоянии $s_i$ искусственный интеллект выполняет поиск $a_i$-го хода, согласно некоторой оценке $r_i$ и получает от окружающей среды новую оценку $r_{i + 1}$, переходя в состояние $s_{i + 1}$ (\refimage{tdschema}).

\image{Схематичное представление процесса обучения с подкреплением}{td-schema}{tdschema}

В процессе реализации идеи возникают такие вопросы как: <<Что считать состоянием?>>, <<Как вычислить оценку?>>, <<Как распространить оценку от конечного состояния к начальному?>>. Существующие методы ответа на эти вопросы влияют на конечный результат, зависят от задачи и описаны в научных трудах \cite{tdlearn}.

Благодаря обширной самостоятельной игре и обучению на своих ошибках, искусственный интеллект достиг уровня игры, который превзошел уровень гроссмейстеров-людей. В 1992 году TD-Gammon выиграла чемпионат мира по нардам, продемонстрировав свое мастерство и стратегическое понимание игры.
